{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GraphETM Dev Notebook",
   "id": "75852b8e51af2279"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T07:30:23.535119Z",
     "start_time": "2025-05-14T07:30:20.409771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Imports\n",
    "# Local\n",
    "from model.graphetm_trainer import GraphETMTrainer\n",
    "\n",
    "# External\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "from typing import Dict, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch_geometric as pyg\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "import wandb\n",
    "\n",
    "### Parameters\n",
    "wandb.login()"
   ],
   "id": "b594b9c64bb75b73",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mloicduch\u001B[0m (\u001B[33mloicduch-mcgill-university\u001B[0m) to \u001B[32mhttps://api.wandb.ai\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T07:30:23.577060Z",
     "start_time": "2025-05-14T07:30:23.570255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Seeds\n",
    "pyg.seed_everything(10) # random, np, torch, torch.cuda"
   ],
   "id": "557db418d1a5cca5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T07:30:23.650141Z",
     "start_time": "2025-05-14T07:30:23.648352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Helper\n",
    "def evaluate_ari(cell_embed, adata):\n",
    "    \"\"\"\n",
    "        This function is used to evaluate ARI using the lower-dimensional embedding\n",
    "        cell_embed of the single-cell data\n",
    "        :param cell_embed: a NxK single-cell embedding generated from NMF or scETM\n",
    "        :param adata: single-cell AnnData data object (default to to mp_anndata)\n",
    "        :return: ARI score of the clustering results produced by Louvain\n",
    "    \"\"\"\n",
    "    adata.obsm['cell_embed'] = cell_embed\n",
    "    sc.pp.neighbors(adata, use_rep=\"cell_embed\", n_neighbors=30)\n",
    "    sc.tl.louvain(adata, resolution=0.15)\n",
    "    ari = adjusted_rand_score(adata.obs['Celltype'], adata.obs['louvain'])\n",
    "    return ari"
   ],
   "id": "4506eb4361744130",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Model\n",
    "Model Implementation:"
   ],
   "id": "9a934ae5e98b2f50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T07:30:23.661535Z",
     "start_time": "2025-05-14T07:30:23.656540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# @title ENCODER\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "        Encoder module for GraphETM.\n",
    "\n",
    "        Attributes:\n",
    "                q_theta: q_theta\n",
    "                theta_act: theta_act\n",
    "                mu_q_theta: mu_q_theta\n",
    "                logsigma_q_theta: logsigma_q_theta\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_topics: int,\n",
    "            vocab_size: int,\n",
    "            encoder_hidden_size: int,\n",
    "            dropout: float = 0.5,\n",
    "            theta_act: str = 'tanh'\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Initialize the Encoder module.\n",
    "\n",
    "            Args:\n",
    "                num_topics: Number of topics.\n",
    "                vocab_size: Size of vocabulary.\n",
    "                encoder_hidden_size: Size of the hidden layer in the encoder.\n",
    "                theta_act: Activation function for theta.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Dropout\n",
    "        self.thres_dropout = dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Theta Activation\n",
    "        self.theta_act = self._get_activation(theta_act)\n",
    "\n",
    "        ## define variational distribution for \\theta_{1:D} via amortization\n",
    "        self.q_theta = nn.Sequential(\n",
    "            nn.Linear(vocab_size, encoder_hidden_size),\n",
    "            self.theta_act,\n",
    "            nn.Linear(encoder_hidden_size, encoder_hidden_size),\n",
    "            self.theta_act,\n",
    "        )\n",
    "        self.mu_q_theta = nn.Linear(encoder_hidden_size, num_topics, bias=True)\n",
    "        self.logsigma_q_theta = nn.Linear(encoder_hidden_size, num_topics, bias=True)\n",
    "\n",
    "    def infer_topic_distribution(self, normalized_bows: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "            Returns a deterministic topic distribution for evaluation purposes bypassing the stochastic reparameterization step.\n",
    "\n",
    "            Args:\n",
    "                normalized_bows (torch.Tensor): Normalized bag-of-words input.\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: Deterministic topic proportions.\n",
    "        \"\"\"\n",
    "        q_theta = self.q_theta(normalized_bows)\n",
    "        mu_theta = self.mu_q_theta(q_theta)\n",
    "        theta = F.softmax(mu_theta, dim=-1)\n",
    "        return theta\n",
    "\n",
    "    def forward(self, bow_norm: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Returns parameters of the variational distribution for \\theta.\n",
    "\n",
    "        Args:\n",
    "            bow_norm: (batch, V) Normalized batch of Bag-of-Words.\n",
    "\n",
    "        Returns:\n",
    "            mu_theta: mu_theta\n",
    "            logsigma_theta: logsigma_theta\n",
    "            kl_theta: kl_theta\n",
    "\n",
    "        \"\"\"\n",
    "        q_theta = self.q_theta(bow_norm)\n",
    "        if self.thres_dropout > 0:\n",
    "            q_theta = self.dropout(q_theta)\n",
    "        mu_theta = self.mu_q_theta(q_theta)\n",
    "        logsigma_theta = self.logsigma_q_theta(q_theta)\n",
    "\n",
    "        # KL[q(theta)||p(theta)] = lnq(theta) - lnp(theta)\n",
    "        kl_theta = -0.5 * torch.sum(1 + logsigma_theta - mu_theta.pow(2) - logsigma_theta.exp(), dim=-1).mean()\n",
    "\n",
    "        return mu_theta, logsigma_theta, kl_theta\n",
    "\n",
    "    def _get_activation(self, act): # TODO: Redundant method.\n",
    "        if act == 'tanh':\n",
    "            act = nn.Tanh()\n",
    "        elif act == 'relu':\n",
    "            act = nn.ReLU()\n",
    "        elif act == 'softplus':\n",
    "            act = nn.Softplus()\n",
    "        elif act == 'rrelu':\n",
    "            act = nn.RReLU()\n",
    "        elif act == 'leakyrelu':\n",
    "            act = nn.LeakyReLU()\n",
    "        elif act == 'elu':\n",
    "            act = nn.ELU()\n",
    "        elif act == 'selu':\n",
    "            act = nn.SELU()\n",
    "        elif act == 'glu':\n",
    "            act = nn.GLU()\n",
    "        else:\n",
    "            print('Defaulting to tanh activations...')\n",
    "            act = nn.Tanh()\n",
    "        return act"
   ],
   "id": "217a9bef26670a4d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T07:30:23.772535Z",
     "start_time": "2025-05-14T07:30:23.768775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# @title DECODER\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "        Decoder module for GraphETM.\n",
    "\n",
    "        Attributes:\n",
    "            rho: Word embedding matrix.\n",
    "            alphas: Topic embedding matrix.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding: torch.Tensor,\n",
    "            num_topics: int,\n",
    "            trainable: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Initialize the Decoder module.\n",
    "\n",
    "            Args:\n",
    "                num_topics: Number of topics.\n",
    "                vocab_size: Size of vocabulary.\n",
    "                rho_size: Size of rho.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Replace word embedding matrix with embeddings derived from the iBKH.\n",
    "        # TODO: 1) Use GCN to process the iBKH and produce graph embeddings (iBKH-embeddings). (DONE)\n",
    "        # TODO: 2) Replace rho in the Decoder with the iBKH-embeddings. (DONE)\n",
    "        # TODO: 3) Pass the iBKH-embeddings through linear -> alpha; followed by softmax -> Beta (beta represents the Decoder weights).\n",
    "        # TODO: 4) (Optional) Add graph reconstruction loss to the training objective.\n",
    "        # TODO: Objective: The latent topic distribution theta for (scRNA and EHR) are multiplied with Beta (essentially grounding the latent topics with the knowledge).\n",
    "\n",
    "        ## define the word embedding matrix \\rho\n",
    "        if trainable: # Trainable\n",
    "            self.embedding = nn.Parameter(embedding.clone()) # V x L\n",
    "        else: # Frozen\n",
    "            self.register_buffer('embedding', embedding.clone()) # V x L\n",
    "\n",
    "        ## define the matrix containing the topic embeddings\n",
    "        self.alphas = nn.Linear(self.embedding.size(1), num_topics, bias=False)\n",
    "\n",
    "    def get_beta(self):\n",
    "        \"\"\"\n",
    "            Retrieve beta by doing softmax over the vocabulary dimension.\n",
    "\n",
    "            Returns:\n",
    "                Beta which represents the topic-word (or topic-feature) distributions.\n",
    "        \"\"\"\n",
    "        logits = self.alphas(self.embedding)\n",
    "        beta = F.softmax(logits, dim=0).transpose(1, 0) # K x V\n",
    "        return beta\n",
    "\n",
    "    def forward(self, theta):\n",
    "        beta = self.get_beta()\n",
    "        preds = torch.log(torch.mm(theta, beta) + 1e-6)\n",
    "        return preds"
   ],
   "id": "37536cf185fb5029",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T07:30:23.806158Z",
     "start_time": "2025-05-14T07:30:23.799656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# @title GraphETM\n",
    "class GraphETM(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            encoder_params: Dict[str, Dict[str, int]],\n",
    "            theta_act: str,\n",
    "            num_topics: int,\n",
    "            embedding_sc: torch.Tensor = None,\n",
    "            embedding_ehr: torch.Tensor = None,\n",
    "            trainable_embeddings = True,\n",
    "            dropout=0.2\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Initialize the ETM model.\n",
    "\n",
    "            Args:\n",
    "                encoder_params: Dictionary of the parameters for the encoders. Dictionary {'sc': {str: Any}, 'ehr': {str: Any}}.\n",
    "                    vocab_size: Size of vocabulary.\n",
    "                    encoder_hidden_size: Size of the hidden layer in the encoder.\n",
    "                theta_act: Activation function for theta.\n",
    "                num_topics: Number of topics.\n",
    "                embedding_sc:  Word embedding rho for single-celled RNA from a knowledge graph.\n",
    "                embedding_ehr: Word embedding rho for Diseases from a knowledge graph.\n",
    "                trainable_embeddings: Whether to fine-tune word embeddings.\n",
    "                dropout: Dropout rate.\n",
    "\n",
    "        \"\"\"\n",
    "        super(GraphETM, self).__init__()\n",
    "\n",
    "        self.encoder_params = encoder_params\n",
    "\n",
    "        self.enc_sc  = Encoder(**encoder_params['sc'],  num_topics=num_topics, dropout=dropout, theta_act=theta_act)\n",
    "        self.enc_ehr = Encoder(**encoder_params['ehr'], num_topics=num_topics, dropout=dropout, theta_act=theta_act)\n",
    "        self.dec_sc  = Decoder(embedding=embedding_sc,  num_topics=num_topics, trainable=trainable_embeddings)\n",
    "        self.dec_ehr = Decoder(embedding=embedding_ehr, num_topics=num_topics, trainable=trainable_embeddings)\n",
    "\n",
    "    # theta ~ mu + std N(0,1)\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "            Returns a sample from a Gaussian distribution via reparameterization.\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul_(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def infer_topic_distribution(self, normalized_bows: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "            Returns a deterministic topic distribution for evaluation purposes bypassing the stochastic reparameterization step.\n",
    "\n",
    "            Args:\n",
    "                normalized_bows (torch.Tensor): Normalized bag-of-words input.\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: Deterministic topic proportions.\n",
    "        \"\"\"\n",
    "        theta = self.encoder.infer_topic_distribution(normalized_bows)\n",
    "        return theta\n",
    "\n",
    "    def _bow_forward(self, encoder, decoder, bow, aggregate=True):\n",
    "        bow_raw  = bow # integer counts\n",
    "        lengths  = bow_raw.sum(1, keepdim=True) + 1e-8\n",
    "        bow_norm = bow_raw / lengths # Normalize\n",
    "\n",
    "        mu, logvar, kld = encoder(bow_norm)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        theta = F.softmax(z, dim=-1)\n",
    "\n",
    "        preds = decoder(theta)\n",
    "        rec_loss = -(preds * bow_raw).sum(1) / lengths.squeeze(1)\n",
    "        if aggregate:\n",
    "            rec_loss = rec_loss.mean()\n",
    "\n",
    "        return {\n",
    "            'rec_loss': rec_loss,\n",
    "            'kl'      : kld,\n",
    "            'theta'   : theta.detach(),\n",
    "            'preds'   : preds.detach(),\n",
    "        }\n",
    "\n",
    "    def forward(self, bow_sc, bow_ehr):\n",
    "        # Encoder-Decoder: ScRNA\n",
    "        output_sc = self._bow_forward(\n",
    "            bow=bow_sc,\n",
    "            encoder=self.enc_sc,\n",
    "            decoder=self.dec_sc)\n",
    "\n",
    "        # Encoder-Decoder: EHR\n",
    "        output_ehr = self._bow_forward(\n",
    "            bow=bow_ehr,\n",
    "            encoder=self.enc_ehr,\n",
    "            decoder=self.dec_ehr)\n",
    "\n",
    "        # Total ELBO Loss\n",
    "        elbo_loss = (output_sc['rec_loss'] + output_ehr['rec_loss']).mean() + output_sc['kl'] + output_ehr['kl']\n",
    "\n",
    "        # Update outputs\n",
    "        output_sc['rec_loss'] = output_sc['rec_loss'].mean()\n",
    "        output_ehr['rec_loss'] = output_ehr['rec_loss'].mean()\n",
    "        return {\n",
    "            'loss': elbo_loss,\n",
    "            'sc' : output_sc,\n",
    "            'ehr': output_ehr,\n",
    "        }"
   ],
   "id": "2c78687568a16648",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Training"
   ],
   "id": "dc5337a766dbb8e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T07:30:23.837070Z",
     "start_time": "2025-05-14T07:30:23.818581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print(f'Using device: {device}')"
   ],
   "id": "253bea9b7ac31024",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T07:30:24.101336Z",
     "start_time": "2025-05-14T07:30:23.853871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Data\n",
    "test_size = 0.2\n",
    "\n",
    "# Load Rho (Graph Embeddings)\n",
    "sc_indices  = np.load('inputs/GraphETM/id_embed_sc.npy')\n",
    "ehr_indices = np.load('inputs/GraphETM/id_embed_ehr.npy')\n",
    "\n",
    "embedding_full = torch.load('inputs/GraphETM/embedding_full.pt', weights_only=False)\n",
    "\n",
    "embedding_sc  = embedding_full[sc_indices,  :]\n",
    "embedding_ehr = embedding_full[ehr_indices, :]\n",
    "\n",
    "# Load Input Data\n",
    "X_sc  = torch.load('inputs/GraphETM/X_sc.pt',  weights_only=False)\n",
    "X_ehr = torch.load('inputs/GraphETM/X_ehr.pt', weights_only=False)\n",
    "\n",
    "X_sc,  X_sc_val  = train_test_split(X_sc , test_size=test_size, random_state=0)\n",
    "X_ehr, X_ehr_val = train_test_split(X_ehr, test_size=test_size, random_state=0)"
   ],
   "id": "e72ba919c7a2536a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T07:32:29.134324Z",
     "start_time": "2025-05-14T07:31:05.826671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Parameters\n",
    "config = {\n",
    "    'model': dict(\n",
    "        theta_act='relu',\n",
    "        num_topics = 50, # K = 50\n",
    "        encoder_params = { ## Encoder parameters\n",
    "            'sc': { # Encoder SC\n",
    "                'vocab_size': X_sc.shape[1],\n",
    "                'encoder_hidden_size': 64\n",
    "            },\n",
    "            'ehr': { # Encoder EHR\n",
    "                'vocab_size': X_ehr.shape[1],\n",
    "                'encoder_hidden_size': 64\n",
    "            }\n",
    "        }, ## Embedding Parameters\n",
    "        embedding_sc  = embedding_sc ,\n",
    "        embedding_ehr = embedding_ehr,\n",
    "        trainable_embeddings = True\n",
    "    ),\n",
    "\n",
    "    'dataloader': dict(\n",
    "        batch_size=32,\n",
    "        shuffle=True\n",
    "    ),\n",
    "\n",
    "    'training': dict(\n",
    "        lr = 0.001,\n",
    "        epochs = 25,\n",
    "    ),\n",
    "\n",
    "    'device': device,\n",
    "}\n",
    "\n",
    "\n",
    "### Model\n",
    "trainer = GraphETMTrainer(\n",
    "    model = GraphETM(**config['model']).to(device), # Model\n",
    "    dataloader_sc  = DataLoader(**config['dataloader'], dataset = X_sc ), # Dataloaders\n",
    "    dataloader_ehr = DataLoader(**config['dataloader'], dataset = X_ehr),\n",
    "    val_dataloader_sc  = DataLoader(**config['dataloader'], dataset = X_sc_val ),\n",
    "    val_dataloader_ehr = DataLoader(**config['dataloader'], dataset = X_ehr_val),\n",
    "    device = device,\n",
    "    wandb_run = wandb.init(\n",
    "        project ='GraphETM',\n",
    "        group = 'GraphETM',\n",
    "        name = f'GraphETM_{int(time.time())}',\n",
    "        config=config, save_code=True) # Start Wandb\n",
    ")\n",
    "\n",
    "### Training\n",
    "trainer.train(\n",
    "    epochs = config['training']['epochs'],\n",
    "    optimizer = optim.Adam(trainer.model.parameters(), lr=config['training']['lr']) # Optimizer\n",
    ")\n",
    "# TODO: Close the trainer.wandb instance."
   ],
   "id": "5976a6a312ce2e02",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/Users/loicduchesne/Library/CloudStorage/OneDrive-Personal/Projects/Graph-scETM/wandb/run-20250514_033105-10yzm3qx</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/loicduch-mcgill-university/GraphETM/runs/10yzm3qx' target=\"_blank\">GraphETM_1747207865</a></strong> to <a href='https://wandb.ai/loicduch-mcgill-university/GraphETM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/loicduch-mcgill-university/GraphETM' target=\"_blank\">https://wandb.ai/loicduch-mcgill-university/GraphETM</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/loicduch-mcgill-university/GraphETM/runs/10yzm3qx' target=\"_blank\">https://wandb.ai/loicduch-mcgill-university/GraphETM/runs/10yzm3qx</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Training GraphETM:   0%|          | 0/25 [00:00<?, ?epoch/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e8393e6dcf04626a4b041d41a4bfc31"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T07:50:23.246039Z",
     "start_time": "2025-05-14T07:50:19.761612Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.wandb.finish()",
   "id": "68b89f2b246487a1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇█████</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>train/ehr/kld</td><td>▅▃▁▄▄▄▄▅▅▅▅▆▅▆▅▆▇▄▆▅▄▅▅▅▅▅▅▆▅█▆▇▅▅▅█▅▇▇▆</td></tr><tr><td>train/ehr/recon_loss</td><td>▇▇▃▃▅▁▃▃▂▃▅▅▂▄▃▇▅▄▄▄▂▁▃▄▂█▄▁▂▄▁▂▃▃▅▃▄▄▃▃</td></tr><tr><td>train/sc/kld</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/sc/recon_loss</td><td>█▇▅▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_loss</td><td>██▂▂▂▂▁▂▂▂▂▂▁▁▂▁▁▂▂▁▂▂▁▂▂▁▂▂▂▂▂▁▂▂▂▂▂▁▂▂</td></tr><tr><td>val/ehr/kld</td><td>▁▄▆▆▆▇▆▇▇▇▇▇▆▇▇▇▇██████▇█</td></tr><tr><td>val/ehr/recon_loss</td><td>█▃▂▂▂▂▂▁▂▁▂▂▁▂▁▂▁▂▂▁▁▁▁▁▂</td></tr><tr><td>val/sc/kld</td><td>█▅▄▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/sc/recon_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/total_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>2149</td></tr><tr><td>epoch</td><td>24</td></tr><tr><td>train/ehr/kld</td><td>0.04152</td></tr><tr><td>train/ehr/recon_loss</td><td>2.4151</td></tr><tr><td>train/sc/kld</td><td>0.0</td></tr><tr><td>train/sc/recon_loss</td><td>6.90927</td></tr><tr><td>train/total_loss</td><td>9.3659</td></tr><tr><td>val/ehr/kld</td><td>0.02905</td></tr><tr><td>val/ehr/recon_loss</td><td>2.32473</td></tr><tr><td>val/sc/kld</td><td>0.0</td></tr><tr><td>val/sc/recon_loss</td><td>6.9006</td></tr><tr><td>val/total_loss</td><td>9.25438</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GraphETM_1747207865</strong> at: <a href='https://wandb.ai/loicduch-mcgill-university/GraphETM/runs/10yzm3qx' target=\"_blank\">https://wandb.ai/loicduch-mcgill-university/GraphETM/runs/10yzm3qx</a><br> View project at: <a href='https://wandb.ai/loicduch-mcgill-university/GraphETM' target=\"_blank\">https://wandb.ai/loicduch-mcgill-university/GraphETM</a><br>Synced 8 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250514_033105-10yzm3qx/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# DONE",
   "id": "381c123766894cba"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
